{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "We talked about how to identify and generate *features* for machine learning algorithms, which are the inputs for the methods and are an extremely important part of the complete machine learning pipeline. Apart from simply using available data $x_1, x_2, \\dots, x_N$ from, say, measurements in an experiment, we can also construct new features as combinations like $x_1 \\cdot x_2$ or functions of the original features like $\\sin(x_1)$. Including these in the analysis can be beneficial for reducing convergence time or even finding a solution at all. A simple example can be tested on the [TensorFlow Playground](https://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.95551&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). When changing the activation to linear (otherwise it will almost always find a suitable mapping) for the $\\text{XOR}$ problem, only using $x_1$ and $x_2$ as the features will not converge at all. Additionally using $\\sin(x_1)$ and $\\sin(x_2)$ creates at least a partial classification of the whole dataset (do you know why this classification does not work? Recall the arguments from last lecture).<br />\n",
    "The number of features is generally referred to as the *dimension* of a problem.\n",
    "\n",
    "A problem that will keep coming up in machine learning settings is how many data points to generate (or measure) for training, or, making the most out of the available data points. This is related to how many features a dataset is comprised of. The more features (dimensions) are in a dataset, the \"exponentially more\" samples are needed to accurately probe or *sample* that space. This is an instance of **the curse of dimensionality**, where adding a linear number of dimensions creates demand for an exponential increase in some resource(s), in this case the number of samples. You can play around with the number of samples used for probing 1D, 2D and 3D space below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f7b83b31c546b496b804c51418e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='data_size', max=500, min=2), Output()), _dom_classes=('wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_samples(data_size=2)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "#from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "samples = np.random.rand(501)\n",
    "\n",
    "def plot_samples(data_size=2):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax0 = fig.add_subplot(131)\n",
    "    ax1 = fig.add_subplot(132)\n",
    "    ax2 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "    ax0.cla()\n",
    "    ax1.cla()\n",
    "    ax2.cla()\n",
    "    \n",
    "    ax0.set_title(\"1D\")\n",
    "    ax1.set_title(\"2D\")\n",
    "    ax2.set_title(\"3D\")\n",
    "\n",
    "    ax0.set_xlim([0,1])\n",
    "    ax0.set_ylim([0,1])\n",
    "    ax1.set_xlim([0,1])\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax2.set_xlim([0,1])\n",
    "    ax2.set_ylim([0,1])\n",
    "    ax2.set_zlim([0,1])\n",
    "    \n",
    "    ax0.hlines(0.5, 0, 1, color='gray', alpha=0.5, lw=4)\n",
    "    ax0.scatter(samples[:data_size], data_size*[0.5], lw=0.1)\n",
    "    \n",
    "    ax1.scatter(samples[:data_size], samples[-data_size:])\n",
    "    \n",
    "    ax2.scatter(samples[:data_size], samples[-data_size:], samples[-10-data_size:-10])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "interact(plot_samples, data_size=(2, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly speaking, when we decide that 10 samples are sufficient for describing the 1D space above, the 2D space would need around 100 samples. For the 3D space, already around 1000 datapoints are needed. This is just a coarse estimate, but here, we'd need $10^d$ samples for $d$ dimensions ($= d$ features) in the dataset. Usually, we're not in the position to get arbitrary amounts of data to probe these spaces, so including more features must be carefully planned with respect to the available data. The situation in most cases is that we're given a fixed number of data points, so adding more dimensions to the problem will make the dataset more *sparse* regarding the problem.\n",
    "\n",
    "A solution, or at least a way to ease this problem, is **dimensionality reduction** or **model order reduction**. Theoretically, adding more features for an analysis should increase the predictive capabilities of machine learning models, but adding features also requires taking a lot more data into the training phase. The idea of dimensionality reduction is to represent the available data by fewer features than originally present. \n",
    "\n",
    "The naive approach would be to simply discard some features and forget about them. This approach is called *feature elimination* and makes sense when it's very obvious that certain features do not play any role in the problem at hand, e.g., temperature for temperature-independet phenomena, or when it doesn't influence the dataset at all. An analytical way to do so is to perform a **principal component analysis**, which we'll come to later today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
