{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "We briefly mentioned a few naive approaches to manipulating the number of features used for describing a problem. Another approach would be to combine features by linear (or later even nonlinear transformations) in a more methodical way, that respects the *correlations* in a dataset. The idea is visualized in the graph below, where two features $x_1$ and $x_2$ are correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def7d64bfafa4b0dbc947a1bc8ff31e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='line'), Checkbox(value=False, description='dists'), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_corr(line, dists)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "x = np.random.rand(40)\n",
    "y = -0.3 + 0.5*x\n",
    "y_test = -0.3 + 0.5*x + 0.2*np.random.rand(40)-0.1\n",
    "p1 = np.array([0.0, -0.3])\n",
    "p2 = np.array([1.0,  0.2])\n",
    "\n",
    "\n",
    "points = np.c_[x,y_test]\n",
    "\n",
    "d=np.cross(p2-p1,points-p1)/np.linalg.norm(p2-p1)\n",
    "\n",
    "dires = np.array([-d[i]*np.array([1/np.sqrt(2), -1/np.sqrt(2)]) for i in range(len(d))])\n",
    "pps = points-dires\n",
    "    \n",
    "def plot_corr(line, dists):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    #ax.set_xlim([])\n",
    "    ax.set_ylim([-0.4, 0.4])\n",
    "    \n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=20)\n",
    "    \n",
    "    ax.scatter(x, y_test)\n",
    "    if line:\n",
    "        ax.plot(x, y, lw=4, zorder=-20)\n",
    "    if dists:\n",
    "        for i in range(d.shape[0]):\n",
    "            ax.plot([points[i,0], pps[i,0]], [points[i,1], pps[i,1]], \\\n",
    "                      color='darkorange', lw=3, alpha=0.7, zorder=-10)\n",
    "        ax.scatter(pps[:,0], pps[:,1], marker='x')\n",
    "    #plt.axes().set_aspect(\"auto\")#1./plt.axes().get_data_ratio())\n",
    "    ax.set_aspect(\"auto\")\n",
    "    \n",
    "interact(plot_corr, line=False, dists=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It is clear that there is a linear trend in the data (the features are correlated) and that we can find a new set of axes (activate line), where the distances of all the points to the new $x^\\prime$-axis are minimized, as in the image above when you activate dists.\n",
    "\n",
    "These kinds of images are correlation plots. Sometimes manually deciding which features to eliminate or combine is possible by examining these correlation plots, but for hundreds of features this isn't really feasible. A technique that performs this combination of features in a way that reduces the overall number of features is principal component analysis (PCA). It tells us which features are the most important and provides the new axes like in the image above. PCA strives to maximize variance, while minimizing covariance of a dataset, so it finds rotated axes that best represent the dataset.\n",
    "\n",
    "In the example above, the distances of the points to the new $x^\\prime$-axis are very small, so to reduce the dimension of the problem, instead of providing the coordinates of each point for both axes, we could provide the projection of the points onto it and ignore the distance in $y^\\prime$-direction, since it is very small.\n",
    "\n",
    "Mathematically, PCA solves the following problem: For a given dataset $X$ with $N$ samples $x_i$ comprised of $F$ features ($X \\in \\mathbb{R}^{N\\times F}$), find an $K\\times F$-matrix $\\mathbf R$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\mathbf x_i^\\prime = \\mathbf R^\\text{T} \\mathbf x_i \n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf x_i^\\prime \\in \\mathbb{R}^K$ and $K \\leq N$. Let's look at another example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f8fb7c73df40a9999181026d514090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='line'), Checkbox(value=False, description='dists'), C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_corr(line, dists, new_axes)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100)\n",
    "y = -0.3 + 0.5*x\n",
    "y_test = -0.3 + 0.5*x + 0.5*np.random.rand(100)-0.25\n",
    "p1 = np.array([0.0, -0.3])\n",
    "p2 = np.array([1.0,  0.2])\n",
    "\n",
    "points = np.c_[x,y_test]\n",
    "\n",
    "d=np.cross(p2-p1,points-p1)/np.linalg.norm(p2-p1)\n",
    "\n",
    "dires = np.array([-d[i]*np.array([0.701, -0.701]) for i in range(len(d))])\n",
    "pps = points-dires\n",
    "    \n",
    "def plot_corr(line, dists, new_axes):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #plt.cla()\n",
    "    \n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "    \n",
    "    ax.set_xlim([-0.3, 1.3])\n",
    "    ax.set_ylim([-0.5, 0.6])\n",
    "    \n",
    "    ax.scatter(x, y_test)\n",
    "            \n",
    "    if line:\n",
    "        ax.plot(x, y, lw=4, zorder=-20)\n",
    "    if dists:\n",
    "        for i in range(d.shape[0]):\n",
    "            ax.plot([points[i,0], pps[i,0]], [points[i,1], pps[i,1]], \\\n",
    "                      color='darkorange', lw=3, alpha=0.7, zorder=-10)\n",
    "        ax.scatter(pps[:,0], pps[:,1], marker='x')\n",
    "    if new_axes:\n",
    "        ax.arrow(0, -0.3, 1, 0.5, head_width=0.05, head_length=0.1, \\\n",
    "         fc='red', ec='red', length_includes_head=True, zorder=1000)\n",
    "        ax.arrow(0, -0.3, -0.701*0.1, 0.701*0.1, head_width=0.02, head_length=0.05, \\\n",
    "         fc='red', ec='red', length_includes_head=True, zorder=1000)\n",
    "        #plt.quiver(0, -0.3, [1, -0.701], [0.5, 0.701], color = 'red', lw = 3, zorder=1000)\n",
    "    \n",
    "    \n",
    "interact(plot_corr, line=False, dists=False, new_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the information in the dataset is captured along the rotated $x^\\prime$-axis, which is the axis along which the variance is maximal. The $y^\\prime$-axis is orthogonal to this axis, maximizing the variance \"perpendicular\" to the first axis. If we project the points onto the $x^\\prime$-axis, we can retain the most information about the original dataset. To accurately do all of this, we need the directions of the axes $x^\\prime$ and $y^\\prime$, as well as their \"lengths\" $\\lambda_1$ and $\\lambda_2$. The length of an axis is the variance of the data along this direction. In this sense, PCA fits a hyperellipsoid to the data.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "To perform a PCA, the data has to be zero-centered first, which means that the mean of each feature over the whole dataset is calculated and subtracted from each respective column of every sample. The mean of feature $f$ is calculated as\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\mu_f = \\frac 1 N \\sum_i^N X_{if}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\mu$ is a vector containing the feature means as columns. The zero-centered sample matrix is then \n",
    "\n",
    "\\begin{equation*}\n",
    "  \\bar{X} = X - U^\\text{T}\n",
    "\\end{equation*}\n",
    "\n",
    "where $U = \\begin{bmatrix} \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} & \\cdots & \\rule[-1ex]{0.5pt}{2.5ex} \\\\ \\mu & \\mu & \\cdots & \\mu \\\\ \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} & \\cdots & \\rule[-1ex]{0.5pt}{2.5ex} \\end{bmatrix}$. The next step is to calculate the covariance matrix $C$ of the data:\n",
    "\n",
    "\\begin{equation*}\n",
    "  C = \\frac 1 N \\bar{X}^\\text{T}\\bar{X}\n",
    "\\end{equation*}\n",
    "\n",
    "Recall that the $F\\times F$ covariance matrix contains the variance of a variable on the diagonal, and the covariances on the off-diagonal elements. When we're trying to maximize variance (or, equivalently, minimize covariance), we're looking for a diagonal matrix. This is the last step; diagonalize the covariance matrix (which is possible, since $C$ is symmetric). This is done by finding the eigenvectors composing the matrix $V$ (more specifically, the *right* eigenvectors), such that\n",
    "\n",
    "\\begin{equation*}\n",
    "  C = VDV^\\text{T}\n",
    "\\end{equation*}\n",
    "\n",
    "where $D$ is a diagonal matrix. The eigenvectors $v_i$ are the **principal components**. In this eigensystem, the entries $\\lambda_i$ of $D$ are the **variance** of each data point in that system. In general, the eigenvalues and eigenvectors aren't sorted. After sorting them in decreasing order of magnitude of the eigenvalues, the reduction process is performed by truncating the last $F-K$ rows of $V$ and $D$, such that only the entries with the lowest variances are discarded:\n",
    "\n",
    "\\begin{equation*}\n",
    "  R = V[0:K]\n",
    "\\end{equation*}\n",
    "\n",
    "This is the reduction matrix $R$ from the problem description. Multiplying the full dataset by this reduction matrix gives the new dataset \n",
    "\n",
    "\\begin{equation*}\n",
    "  X^\\prime = \\bar{X}R \\in \\mathbb{R}^{N\\times K}\n",
    "\\end{equation*}\n",
    "\n",
    "with a reduced number $K$ of features. The first row of this dataset contains the projection of the first sample in the original dataset onto the first principal component, and so on. To get the embedding of the original data into this lower-dimensional subspace, the means $U$ have to be added again to complete the transformation.\n",
    "\n",
    "### PCA by SVD\n",
    "\n",
    "In practice, calculating and diagonalizing $C$ is expensive and a much easier method is to use *singular value decomposition* on the data matrix itself:\n",
    "\n",
    "\\begin{equation*}\n",
    "  X = USV^\\text{T}\n",
    "\\end{equation*}\n",
    "\n",
    "such that \n",
    "\n",
    "\\begin{equation*}\n",
    "  C = \\frac 1 N X^\\text{T}X = \\frac 1 N VS^\\text{T}U^\\text{T}USV^\\text{T} = \\frac 1 N V S^2 V^\\text{T}\n",
    "\\end{equation*}\n",
    "\n",
    "Comparing this to the result from above, we see that the right-singular eigenvectors in $V$ are the principal directions, and the variances are $\\lambda_i = \\frac{s_i^2}{N}$. You can see this by multiplying the equation above with $V$ from the right.<br />\n",
    "The columns of $US$ are the principal components. Truncating all matrices like we saw in the last lesson to the first $K$ entries to compute \n",
    "\n",
    "\\begin{equation*}\n",
    "  X^\\prime = U[0:K]S[0:K]V[0:K]^\\text{T}\n",
    "\\end{equation*}\n",
    "\n",
    "gives the *reduced* representation of the data, which now only consists of the $K$ most important feature combinations regarding the variance they explain for the original dataset.\n",
    "\n",
    "### Summary\n",
    "PCA is extremely common in dimensionality reduction. It can be used as a way to visualize a very high-dimensional dataset in 2D or 3D. See for example the 1D, 2D, and 3D PCA representation of *Andersen's Iris dataset* we've briefly seen in lecture 01, but is also used as a preprocessing step for machine learning algorithms. As we've seen in the last lesson about SVD, it can be applied to images to retain only the most important information about an image to reduce the data size and to be able to use much smaller deep learning models. It improves the performance of many machine learning (or classical) algorithms, since it minimizes correlations between the new axes and, if desired, removes unwanted features which are most likely noise in the dataset. The newly found axes are still orthogonal. In image-processing, this transformation is sometimes also called the *Karhunen-Loève transformation*.\n",
    "\n",
    "PCA can be generalized as principal curve analysis or principal surface analysis, and very commonly *kernel-PCA* which is a nonlinear version of PCA using the kernel-trick."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
